{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"Copy of AML_9_Modelling_ImprovePerfAlgoTuning.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"TdLnzL-8VYg2"},"source":["# Improve Performance with Algorithm Tuning"]},{"cell_type":"markdown","metadata":{"id":"JpZsPEujVYg4"},"source":["**ML models are parameterized** so that their behavior can be tuned for a given problem. Models can have many parameters and **finding the best combination of parameters** is an optimization task and can be treated as a search problem (more later).\n","\n","We will see how to tune the parameters of ML algorithms in Python using scikit-learn. \n","\n","The goal is to learn:\n","1. The importance of algorithm parameter tuning to improve algorithm performance\n","2. How to use a grid search algorithm tuning strategy\n","3. How to use a random search algorithm tuning strategy."]},{"cell_type":"markdown","metadata":{"id":"VlCigFSTVYg5"},"source":["# ML Algorithm hyper-parameters optimization"]},{"cell_type":"markdown","metadata":{"id":"ADK0eDXWVYg7"},"source":["Algorithm tuning is a final step in the process of applied ML before finalizing your model. It is sometimes called ***hyperparameter optimization*** where:\n","\n","* the _algorithm parameters_ are referred to as **hyperparameters**\n","* the _coefficients_ found by the ML algorithm itself are referred to as **parameters**. \n","\n","Optimization suggests the search-nature of the problem. Phrased as a search problem, you can use different search strategies to find a good and robust parameter or set of parameters for an algorithm on a given problem. \n","\n","Python scikit-learn provides 2 simple methods for algorithm parameter tuning:\n","1. Grid Search parameter tuning\n","2. Random Search parameter tuning."]},{"cell_type":"markdown","metadata":{"id":"USjW1RvPW07I"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"m32ckL3sW0wt","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1649389791357,"user_tz":-120,"elapsed":876,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"49462c27-e824-4b5c-d72e-ac9e4545f755"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AMLBas2122/main/datasets/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     preg  plas  pres  skin  test  mass   pedi  age  class\n","0       6   148    72    35     0  33.6  0.627   50      1\n","1       1    85    66    29     0  26.6  0.351   31      0\n","2       8   183    64     0     0  23.3  0.672   32      1\n","3       1    89    66    23    94  28.1  0.167   21      0\n","4       0   137    40    35   168  43.1  2.288   33      1\n","..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n","763    10   101    76    48   180  32.9  0.171   63      0\n","764     2   122    70    27     0  36.8  0.340   27      0\n","765     5   121    72    23   112  26.2  0.245   30      0\n","766     1   126    60     0     0  30.1  0.349   47      1\n","767     1    93    70    31     0  30.4  0.315   23      0\n","\n","[768 rows x 9 columns]"],"text/html":["\n","  <div id=\"df-09c5bd02-788e-42a6-bfa9-f1bbcbe8b4cf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>preg</th>\n","      <th>plas</th>\n","      <th>pres</th>\n","      <th>skin</th>\n","      <th>test</th>\n","      <th>mass</th>\n","      <th>pedi</th>\n","      <th>age</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>763</th>\n","      <td>10</td>\n","      <td>101</td>\n","      <td>76</td>\n","      <td>48</td>\n","      <td>180</td>\n","      <td>32.9</td>\n","      <td>0.171</td>\n","      <td>63</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>764</th>\n","      <td>2</td>\n","      <td>122</td>\n","      <td>70</td>\n","      <td>27</td>\n","      <td>0</td>\n","      <td>36.8</td>\n","      <td>0.340</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>765</th>\n","      <td>5</td>\n","      <td>121</td>\n","      <td>72</td>\n","      <td>23</td>\n","      <td>112</td>\n","      <td>26.2</td>\n","      <td>0.245</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>766</th>\n","      <td>1</td>\n","      <td>126</td>\n","      <td>60</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.1</td>\n","      <td>0.349</td>\n","      <td>47</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>767</th>\n","      <td>1</td>\n","      <td>93</td>\n","      <td>70</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>30.4</td>\n","      <td>0.315</td>\n","      <td>23</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>768 rows Ã— 9 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09c5bd02-788e-42a6-bfa9-f1bbcbe8b4cf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-09c5bd02-788e-42a6-bfa9-f1bbcbe8b4cf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-09c5bd02-788e-42a6-bfa9-f1bbcbe8b4cf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"lhvddoOsVYg-"},"source":["## 1. Grid Search Parameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"FivKEQX0VYg_"},"source":["Grid search is an approach to parameter tuning that will **methodically build and evaluate a model for each combination of algorithm parameters specified in a grid**. \n","\n","You can perform a grid search using the GridSearchCV class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n","\n","The example below evaluates different alpha values for the Ridge Regression algorithm on the standard diabetes dataset. This is a one-dimensional grid search."]},{"cell_type":"code","metadata":{"id":"8-sb9YjPVYhK","executionInfo":{"status":"ok","timestamp":1649389792470,"user_tz":-120,"elapsed":1117,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["import numpy as np\n","#\n","#from pandas import read_csv\n","#\n","from sklearn.linear_model import Ridge\n","#\n","from sklearn.model_selection import GridSearchCV                   # <--"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSarf4FZVYiT","executionInfo":{"status":"ok","timestamp":1649389792471,"user_tz":-120,"elapsed":5,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"bAos0D0LVYig","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649389792472,"user_tz":-120,"elapsed":6,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"a1db7713-cfa0-4d36-89e9-85d9ad2cdf35"},"source":["# Grid Search for Algorithm Tuning\n","alphas = np.array([24, 6, 0.3, 0.002])\n","param_grid = dict(alpha=alphas)\n","model = Ridge()\n","grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n","grid.fit(X, Y)\n","print(grid.best_score_)\n","print(grid.best_estimator_.alpha)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["0.2762337607493569\n","6.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"jezfluyqVYij"},"source":["Running the example lists out the optimal score achieved and the set of parameters in the\n","grid that achieved that score. In this case ap optimal `alpha` among those explicitly given is found."]},{"cell_type":"markdown","metadata":{"id":"XrjG_ElKZw-o"},"source":["## <font color='red'>Exercise 1</font>"]},{"cell_type":"markdown","metadata":{"id":"SMZb8HVOZw6Q"},"source":["Can you find better values than the one found above? How would you do it?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a_4DSjaSZwun"},"source":["## <font color='green'>Solution 1</font>"]},{"cell_type":"code","metadata":{"id":"ShoiNulTZwTM","executionInfo":{"status":"ok","timestamp":1649389792748,"user_tz":-120,"elapsed":280,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["# put your code here"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"6h3lYSBdPHKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649389792749,"user_tz":-120,"elapsed":17,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"b4f4e51e-1d8c-4926-a922-a826f93f5da4"},"source":["# I use brute force, somehow: \n","#\n","# first attempt: [5000, 500, 50, 5, 0.5, 0.005]\n","# second attempt: [10, 5, 1, 0.5, 0.25]\n","# ...\n","\n","my_choices = np.array([10, 5, 1, 0.5, 0.25])\n","\n","# Grid Search for Algorithm Tuning\n","alphas = my_choices\n","param_grid = dict(alpha=alphas)\n","model = Ridge()\n","grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n","grid.fit(X, Y)\n","print(grid.best_score_)\n","print(grid.best_estimator_.alpha)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["0.27624982190409064\n","10.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"hDebgFIaVYik"},"source":["## Random Search parameter tuning"]},{"cell_type":"markdown","metadata":{"id":"x1IgugYyVYik"},"source":["Random search is an approach to parameter tuning that will **sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations**. \n","\n","A model is constructed and evaluated for each combination of parameters chosen. You can perform a random search for algorithm parameters using the `RandomizedSearchCV` class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)).\n","\n","Suppose that you want to go slow, your algo does not impose time constraints in training time, so you do not require an optimal, but perhaps too aggressive, alpha. How do you find a value in the [0,1] interval?\n"]},{"cell_type":"code","metadata":{"id":"GIdZoLSG_4hY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649389792750,"user_tz":-120,"elapsed":14,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"18533a8b-cb3b-48df-8047-3ade8ca42bd4"},"source":["# Grid Search for Algorithm Tuning\n","alphas = np.array([1., 0.1, 0.01, 0.001])\n","param_grid = dict(alpha=alphas)\n","model = Ridge()\n","grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n","grid.fit(X, Y)\n","print(grid.best_score_)\n","print(grid.best_estimator_.alpha)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.27610844129292433\n","1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"ws_Cni8YALaL"},"source":["Well, sure, you found `1.0` is optimal, but just among the ones you typed. Which is the best floating point value in the [0,1] interval is still unknown.. and you cannot type manually all real values, of course! So, what?\n","\n","The example below evaluates different random alpha values between 0 and 1 for the Ridge Regression algorithm on the standard diabetes dataset. A total of 100 iterations are performed with uniformly random alpha values selected in the range between 0 and 1 (the range that alpha values can take)."]},{"cell_type":"code","metadata":{"id":"6lFraTa8VYil","executionInfo":{"status":"ok","timestamp":1649389792750,"user_tz":-120,"elapsed":12,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["from scipy.stats import uniform\n","#\n","from sklearn.linear_model import Ridge\n","#\n","from sklearn.model_selection import RandomizedSearchCV                   # <--"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuB0_9b7VYis","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649389810734,"user_tz":-120,"elapsed":17996,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"1fd6d114-f29b-4101-8dd5-c6bdd638ced6"},"source":["# Randomized for Algorithm Tuning\n","param_grid = { 'alpha' : uniform()}\n","model = Ridge()\n","rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=1000, random_state=7)\n","rsearch.fit(X, Y)\n","print(rsearch.best_score_)\n","print(rsearch.best_estimator_.alpha)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0.2761084305906956\n","0.9997330141673058\n"]}]},{"cell_type":"markdown","metadata":{"id":"NCASRtmiVYiv"},"source":["Running the example produces results much like those in the grid search example above. An\n","optimal `alpha` value near `1.0` is discovered."]},{"cell_type":"markdown","metadata":{"id":"SS76zGlRVYiw"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"cYHAUIXWVYix"},"source":["What we did:\n","\n","* we discovered that algorithm parameter tuning is an important step for improving algorithm performance right before presenting results or preparing a system for production. We explored two methods that you can use right now in Python and scikit-learn to improve your algorithm results (Grid Search Parameter Tuning, Random Search Parameter Tuning)."]}]}